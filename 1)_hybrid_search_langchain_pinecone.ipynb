{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet pinecone-client pinecone-text pinecone-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"your secret key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain_community)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Using cached SQLAlchemy-2.0.31-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.7 (from langchain_community)\n",
      "  Downloading langchain-0.2.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain_community)\n",
      "  Downloading langchain_core-0.2.20-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
      "  Downloading langsmith-0.1.88-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain_community)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.7->langchain_community)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain<0.3.0,>=0.2.7->langchain_community)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "     ---------------------------------------- 0.0/125.2 kB ? eta -:--:--\n",
      "     ------------- -------------------------- 41.0/125.2 kB ? eta -:--:--\n",
      "     ---------------------------- ---------- 92.2/125.2 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------- --- 112.6/125.2 kB 939.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 125.2/125.2 kB 611.4 kB/s eta 0:00:00\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.12->langchain_community)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (24.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n",
      "  Downloading orjson-3.10.6-cp310-none-win_amd64.whl.metadata (51 kB)\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.6/51.6 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain_community)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community)\n",
      "  Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.2 MB 655.8 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/2.2 MB 655.8 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/2.2 MB 620.6 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/2.2 MB 620.6 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 580.1 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 580.1 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 655.9 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 655.9 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.2 MB 655.1 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.2 MB 655.1 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 654.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 654.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 654.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 603.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 603.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 603.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 603.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 491.5 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.5/2.2 MB 491.5 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 507.8 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 507.8 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 525.8 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 525.8 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.7/2.2 MB 560.6 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.7/2.2 MB 560.6 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.8/2.2 MB 574.2 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.8/2.2 MB 574.2 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.9/2.2 MB 586.5 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.9/2.2 MB 587.1 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.9/2.2 MB 587.1 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 603.9 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.0/2.2 MB 618.6 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.2 MB 619.5 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.2 MB 619.5 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.1/2.2 MB 626.7 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.1/2.2 MB 626.7 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.2/2.2 MB 633.6 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.2/2.2 MB 633.6 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 640.0 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 640.0 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.4/2.2 MB 640.6 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.4/2.2 MB 640.6 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 641.5 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 641.5 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.5/2.2 MB 646.5 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.5/2.2 MB 655.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.5/2.2 MB 655.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.6/2.2 MB 651.0 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.6/2.2 MB 651.0 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.7/2.2 MB 659.5 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.2 MB 659.5 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 667.1 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 667.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 670.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 670.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 669.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 669.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 680.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 680.1 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 679.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 679.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 679.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 681.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 681.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 687.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 687.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 673.4 kB/s eta 0:00:00\n",
      "Using cached aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain-0.2.9-py3-none-any.whl (987 kB)\n",
      "   ---------------------------------------- 0.0/987.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/987.7 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 122.9/987.7 kB 7.0 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 133.1/987.7 kB 7.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 194.6/987.7 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 276.5/987.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 337.9/987.7 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 399.4/987.7 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 409.6/987.7 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 532.5/987.7 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 532.5/987.7 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 675.8/987.7 kB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 675.8/987.7 kB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 757.8/987.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 757.8/987.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 931.8/987.7 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 931.8/987.7 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  983.0/987.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 987.7/987.7 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.20-py3-none-any.whl (371 kB)\n",
      "   ---------------------------------------- 0.0/371.7 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 61.4/371.7 kB 3.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 61.4/371.7 kB 3.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 194.6/371.7 kB 1.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 194.6/371.7 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 266.2/371.7 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 266.2/371.7 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 348.2/371.7 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 348.2/371.7 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 371.7/371.7 kB 856.5 kB/s eta 0:00:00\n",
      "Downloading langsmith-0.1.88-py3-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.3 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 61.4/134.3 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 61.4/134.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.3/134.3 kB 1.3 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "Using cached SQLAlchemy-2.0.31-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "Using cached greenlet-3.0.3-cp310-cp310-win_amd64.whl (292 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.6-cp310-none-win_amd64.whl (136 kB)\n",
      "   ---------------------------------------- 0.0/136.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/136.4 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 102.4/136.4 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 102.4/136.4 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 102.4/136.4 kB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- 136.4/136.4 kB 621.5 kB/s eta 0:00:00\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "   ---------------------------------------- 0.0/423.9 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 92.2/423.9 kB 2.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 92.2/423.9 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 174.1/423.9 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 174.1/423.9 kB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 276.5/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 276.5/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 409.6/423.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- 423.9/423.9 kB 562.8 kB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.9 MB 6.8 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/1.9 MB 6.8 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/1.9 MB 1.8 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/1.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 985.7 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 985.7 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/1.9 MB 1.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/1.9 MB 1.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/1.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.6/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.6/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.7/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.7/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.7/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 996.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 984.5 kB/s eta 0:00:00\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: typing-inspect, tenacity, PyYAML, pydantic-core, orjson, multidict, marshmallow, jsonpointer, greenlet, frozenlist, async-timeout, annotated-types, yarl, SQLAlchemy, pydantic, jsonpatch, dataclasses-json, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.9 langchain-core-0.2.20 langchain-text-splitters-0.2.2 langchain_community-0.2.7 langsmith-0.1.88 marshmallow-3.21.3 multidict-6.0.5 orjson-3.10.6 pydantic-2.8.2 pydantic-core-2.20.1 tenacity-8.5.0 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "# PineconeHybridSearchRetriever this class is responsibe for semantic search and syntactic search  \n",
    "# as you know keyword search and similar search , so it has combination of sparse matrix and dense matrix \n",
    "# and for that we have to create a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the index in the pinecone\n",
    "\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "index_name = \"hybridsearch\"\n",
    "\n",
    "# initialize the pinecone client\n",
    "pc = Pinecone(api_key = api_key)\n",
    "\n",
    "# create the index\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384, # dimension of the dense vector, the reason behind it hugging face embedding techniques i am using\n",
    "                       # which is nothing but sentence transormers that by default converts any text into 384 dimensions \n",
    "        metric='dotproduct', # sparse value supported only for dotproduct. \n",
    "        num_shards=1,\n",
    "        replicas=1,\n",
    "        server=ServerlessSpec(cloud = 'aws', region = \"us-east-1\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x1adca8f9a50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain_huggingface)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain_huggingface) (0.2.20)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain_huggingface)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain_huggingface)\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 43.6/43.6 kB 710.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.88)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.3.1+cu121)\n",
      "Requirement already satisfied: numpy in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.0)\n",
      "Requirement already satisfied: Pillow in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.39.0->langchain_huggingface)\n",
      "  Using cached safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.4.0)\n",
      "Requirement already satisfied: colorama in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\mlops\\mlops_all_in_one\\venv_mlops\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
      "Downloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "   ---------------------------------------- 0.0/402.8 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 92.2/402.8 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  399.4/402.8 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 402.8/402.8 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 0.0/227.1 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 81.9/227.1 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 92.2/227.1 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------- ----------------- 122.9/227.1 kB 804.6 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 163.8/227.1 kB 821.4 kB/s eta 0:00:01\n",
      "   -------------------------------------  225.3/227.1 kB 860.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 227.1/227.1 kB 867.3 kB/s eta 0:00:00\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.3 MB 5.1 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.2/9.3 MB 2.5 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/9.3 MB 1.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/9.3 MB 3.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/9.3 MB 2.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/9.3 MB 2.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/9.3 MB 3.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/9.3 MB 3.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/9.3 MB 3.0 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.2/9.3 MB 2.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.2/9.3 MB 2.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.4/9.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.5/9.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.6/9.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.7/9.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/9.3 MB 2.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/9.3 MB 2.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.9/9.3 MB 2.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.0/9.3 MB 2.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.1/9.3 MB 2.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.3/9.3 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/9.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.5/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.6/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.7/9.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.8/9.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.9/9.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.0/9.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.2/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.3/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.5/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.6/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.7/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.8/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.1/9.3 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.2/9.3 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.4/9.3 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.5/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.6/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.7/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.8/9.3 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.9/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.0/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.2/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.3/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.5/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.6/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.6/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.7/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.9/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.1/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.2/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.3/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.5/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.5/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.6/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.7/9.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.9/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.2/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.5/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.4/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.5/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.8/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.0/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.1/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 2.6 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers, langchain_huggingface\n",
      "Successfully installed huggingface-hub-0.23.5 langchain_huggingface-0.0.3 safetensors-0.4.3 sentence-transformers-3.0.1 tokenizers-0.19.1 transformers-4.42.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector embeddng and sparse matrix\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['hybrid_lang_pinecone_token'] = os.getenv('hybrid_lang_pinecone_token')\n",
    "\n",
    "# using huggingface embedding techniques\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone_text.sparse.bm25_encoder.BM25Encoder at 0x1ada2ec8880>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this embeddings technics used to created a densed vectors\n",
    "# for my sparse matrix\n",
    "# this is used sparse encoder tf-idf techniques\n",
    "\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "bm25_encoder = BM25Encoder().default()\n",
    "bm25_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 86.27it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"In 2023, I visited Paris\",\n",
    "    \"In 2022, I visited Newyork\",\n",
    "    \"In 2021, I  Visited Orleans\",\n",
    "]\n",
    "\n",
    "# tfidf vallues on these sentence\n",
    "bm25_encoder.fit(sentences)\n",
    "# store the values\n",
    "bm25_encoder.dump(\"bm25_values.json\")\n",
    "\n",
    "# to load the values\n",
    "bm25_encoder = BM25Encoder().load(\"bm25_values.json\")\n",
    "\n",
    "# we created the sparse matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this retriver techniques supports to both vector dense embeddings techniques along with the sparse matrix \n",
    "retriever = PineconeHybridSearchRetriever(embeddings=embeddings, sparse_encoder= bm25_encoder, index = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PineconeHybridSearchRetriever(embeddings=HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False), sparse_encoder=<pinecone_text.sparse.bm25_encoder.BM25Encoder object at 0x000001ADA337CF70>, index=<pinecone.data.index.Index object at 0x000001ADCA8F9A50>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLOPS\\MLOPS_All_In_One\\venv_mlops\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever.add_texts(\n",
    "    [\n",
    "        \"In 2023, I visited Paris\",\n",
    "        \"In 2022, I visited Newyork\",\n",
    "        \"In 2021, I  Visited Orleans\",\n",
    "        \"In 2020, I visited Tokyo\",\n",
    "        \"In 2019, I visited Sydney\",\n",
    "        \"In 2018, I visited Rome\",\n",
    "        \"In 2017, I visited London\",\n",
    "        \"In 2016, I visited Berlin\",\n",
    "        \"In 2015, I visited Moscow\",\n",
    "        \"In 2014, I visited Dublin\",\n",
    "        \"In 2013, I visited Vienna\",\n",
    "        \"In 2012, I visited Budapest\",\n",
    "        \"In 2011, I visited Amsterdam\",\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In 2017, I visited London'),\n",
       " Document(page_content='In 2016, I visited Berlin'),\n",
       " Document(page_content='In 2011, I visited Amsterdam'),\n",
       " Document(page_content='In 2015, I visited Moscow')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"which city i visited recently\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
